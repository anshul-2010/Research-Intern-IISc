{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb61a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c2d07",
   "metadata": {},
   "source": [
    "### Gradient Reversal Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5bdee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReverseLayer(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)*alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg()*ctx.alpha\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150f1ee",
   "metadata": {},
   "source": [
    "### The three models: Feature Extractor, Class Classifier, Domain CLassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7022aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential()\n",
    "        self.feature_extractor.add_module('conv1', nn.Conv2d(3, 64, kernel_size=5)),\n",
    "        self.feature_extractor.add_module('batchnorm1', nn.BatchNorm2d(64)),\n",
    "        self.feature_extractor.add_module('maxpool1', nn.MaxPool2d(2)),\n",
    "        self.feature_extractor.add_module('relu1', nn.ReLU(True)),\n",
    "        self.feature_extractor.add_module('conv2', nn.Conv2d(64, 50, kernel_size=5)),\n",
    "        self.feature_extractor.add_module('batchnorm2', nn.BatchNorm2d(50)),\n",
    "        self.feature_extractor.add_module('drop1', nn.Dropout2d())\n",
    "        self.feature_extractor.add_module('maxpool2', nn.MaxPool2d(2)),\n",
    "        self.feature_extractor.add_module('relu2', nn.ReLU(True))\n",
    "        \n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('fc1', nn.Linear(50*4*4, 100)),\n",
    "        self.class_classifier.add_module('batchnorm1', nn.BatchNorm1d(100)),\n",
    "        self.class_classifier.add_module('relu1', nn.ReLU(True)),\n",
    "        self.class_classifier.add_module('drop1', nn.Dropout2d()),\n",
    "        self.class_classifier.add_module('fc2', nn.Linear(100, 100)),\n",
    "        self.class_classifier.add_module('batchnorm2', nn.BatchNorm1d(100)),\n",
    "        self.class_classifier.add_module('relu2', nn.ReLU(True)),\n",
    "        self.class_classifier.add_module('fc3', nn.Linear(100, 10)),\n",
    "        # self.class_classifier.add_module('softmax', nn.LogSoftmax()),\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('fc1', nn.Linear(50*4*4, 100)),\n",
    "        self.domain_classifier.add_module('batchnorm1', nn.BatchNorm1d(100)),\n",
    "        self.domain_classifier.add_module('relu1', nn.ReLU(True)),\n",
    "        self.domain_classifier.add_module('fc2', nn.Linear(100, 2)),\n",
    "        # self.domain_classifier.add_module('softmax', nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x, alpha):\n",
    "        # x = x.expand(x.data.shape[0], 3, 28, 28)\n",
    "        feature = self.feature_extractor(x)\n",
    "        feature = feature.view(-1, 50*4*4)\n",
    "        reverse = GradientReverseLayer.apply(feature, alpha)\n",
    "        class_output = self.class_classifier(feature)\n",
    "        domain_output = self.domain_classifier(reverse)\n",
    "        class_output = class_output.view(class_output.shape[0], -1)\n",
    "        domain_output = domain_output.view(domain_output.shape[0], -1)\n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a5df91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DANN(\n",
       "  (feature_extractor): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "    (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (class_classifier): Sequential(\n",
       "    (fc1): Linear(in_features=800, out_features=100, bias=True)\n",
       "    (batchnorm1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (drop1): Dropout2d(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (batchnorm2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       "  (domain_classifier): Sequential(\n",
       "    (fc1): Linear(in_features=800, out_features=100, bias=True)\n",
       "    (batchnorm1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DANN()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280ab60",
   "metadata": {},
   "source": [
    "#### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e32d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Pixel Value: 0 \n",
      "Max Pixel Value: 255\n",
      "Mean Pixel Value 33.31842041015625 \n",
      "Pixel Values Std: 78.56748962402344\n",
      "Scaled Mean Pixel Value 0.13066047430038452 \n",
      "Scaled Pixel Values Std: 0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST        \n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "print('Min Pixel Value: {} \\nMax Pixel Value: {}'.format(trainset.data.min(), trainset.data.max()))\n",
    "print('Mean Pixel Value {} \\nPixel Values Std: {}'.format(trainset.data.float().mean(), trainset.data.float().std()))\n",
    "print('Scaled Mean Pixel Value {} \\nScaled Pixel Values Std: {}'.format(trainset.data.float().mean() / 255, trainset.data.float().std() / 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c07dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda = True\n",
    "# lr = 1e-3\n",
    "# image_size = 28\n",
    "\n",
    "\n",
    "# img_transform_source = transforms.Compose([\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "# ])\n",
    "# img_transform_target = transforms.Compose([\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean = (0.5,0.5,0.5), std = (0.5,0.5,0.5))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ec4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f0e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_channels(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "source_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                       transforms.Lambda(repeat_channels)\n",
    "                   ])),\n",
    "    batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "source_test = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (0.5,)),\n",
    "                       transforms.Lambda(repeat_channels)\n",
    "                   ])),\n",
    "    batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d76be94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLoader(data.Dataset):\n",
    "    def __init__(self, data_root, data_list, transform=None):\n",
    "        self.root = data_root\n",
    "        self.transform = transform\n",
    "\n",
    "        f = open(data_list, 'r')\n",
    "        data_list = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        self.n_data = len(data_list)\n",
    "\n",
    "        self.img_paths = []\n",
    "        self.img_labels = []\n",
    "\n",
    "        for data in data_list:\n",
    "            self.img_paths.append(data[:-3])\n",
    "            self.img_labels.append(data[-2])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_paths, labels = self.img_paths[item], self.img_labels[item]\n",
    "        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            imgs = self.transform(imgs)\n",
    "            labels = int(labels)\n",
    "\n",
    "        return imgs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1abc185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=28\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.RandomCrop((image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5),\n",
    "                         std=(0.5,0.5,0.5))\n",
    "])\n",
    "train_list = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\mnist_m\\\\mnist_m\\\\mnist_m_train_labels.txt'\n",
    "dataset_train_target = GetLoader(\n",
    "    data_root='C:\\\\Users\\\\Dell\\\\Downloads\\\\mnist_m\\\\mnist_m\\\\mnist_m_train',\n",
    "    data_list=train_list,\n",
    "    transform=img_transform\n",
    ")\n",
    "test_list = 'C:\\\\Users\\\\Dell\\\\Downloads\\\\mnist_m\\\\mnist_m\\\\mnist_m_test_labels.txt'\n",
    "dataset_test_target = GetLoader(\n",
    "    data_root='C:\\\\Users\\\\Dell\\\\Downloads\\\\mnist_m\\\\mnist_m\\\\mnist_m_test',\n",
    "    data_list=test_list,\n",
    "    transform=img_transform\n",
    ")\n",
    "target_train = torch.utils.data.DataLoader(dataset_train_target,batch_size=512, shuffle=True,num_workers=4)\n",
    "target_test = torch.utils.data.DataLoader(dataset_test_target,batch_size=512, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bba99",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f85bcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7d449f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = DANN()\n",
    "optimizer = optim.SGD(model.parameters(), lr= 0.01, momentum= 0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def optimizer_scheduler(optimizer, p):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75\n",
    "    return optimizer\n",
    "\n",
    "loss_class = torch.nn.NLLLoss()\n",
    "loss_domain = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f600f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in source_test:\n",
    "            data, target = data, target\n",
    "            output, _ = model(data,0.5)\n",
    "            test_loss += float(criterion(output, target))  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += float(pred.eq(target.view_as(pred)).sum())\n",
    "\n",
    "    test_loss /= len(source_test.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(source_test.dataset),\n",
    "        100. * correct / len(source_test.dataset)))\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in target_train:\n",
    "            data, target = data, target\n",
    "            output, _ = model(data,0.5)\n",
    "            test_loss += float(criterion(output, target))  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += float(pred.eq(target.view_as(pred)).sum())\n",
    "\n",
    "    test_loss /= len(target_train.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(target_train.dataset),\n",
    "        100. * correct / len(target_train.dataset)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in target_test:\n",
    "            data, target = data, target\n",
    "            output, _ = model(data,0.5)\n",
    "            test_loss += float(criterion(output, target))  # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += float(pred.eq(target.view_as(pred)).sum())\n",
    "\n",
    "    test_loss /= len(target_test.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(target_test.dataset),\n",
    "        100. * correct / len(target_test.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a2ccb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 469\n"
     ]
    }
   ],
   "source": [
    "print(len(target_train),len(source_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96da809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allepoch=100\n",
    "\n",
    "for epoch in range(allepoch):\n",
    "    len_dataloader = min(len(source_train), len(target_train))\n",
    "    total_steps = allepoch * len(source_train)\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data_source, data_target) in enumerate(zip(source_train, target_train)):\n",
    "        start_time = time.time()\n",
    "        s_img, s_label = data_source\n",
    "\n",
    "        start_steps = epoch * len(source_train)\n",
    "\n",
    "        p = float(i + start_steps) / total_steps\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        optimizer = optimizer_scheduler(optimizer, p)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        batch_size = len(s_label)\n",
    "\n",
    "        domain_label = torch.zeros(batch_size)\n",
    "        domain_label = domain_label.long()\n",
    "\n",
    "\n",
    "        a,b = model(s_img,alpha)\n",
    "        err_s_label = criterion(a, s_label)\n",
    "        err_s_domain = criterion(b, domain_label)\n",
    "\n",
    "        # training model using target data\n",
    "        t_img, _ = data_target\n",
    "\n",
    "        batch_size = len(t_img)\n",
    "\n",
    "        domain_label = torch.ones(batch_size)\n",
    "        domain_label = domain_label.long()\n",
    "\n",
    "\n",
    "\n",
    "        _, b = model(t_img,alpha)\n",
    "        err_t_domain = criterion(b, domain_label)\n",
    "        err = err_s_label + err_s_domain + err_t_domain\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if(i % 1000 == 0):\n",
    "            print('epoch:{},[{}/{}],s_label:{:.3f},s_domain:{:.3f},t_domain:{:.3f},time{}'.\n",
    "                      format(epoch, i, len_dataloader, float(err_s_label), float(err_s_domain),\n",
    "                             float(err_t_domain), time.time() - start_time))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26612022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
